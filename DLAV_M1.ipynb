{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf5df1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:33:27.850713Z",
     "start_time": "2022-04-27T14:33:22.826849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "import argparse, time, logging, os, math, tqdm, cv2\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, image\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gluoncv as gcv\n",
    "from gluoncv import data\n",
    "from gluoncv.data import mscoco\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.data.transforms.pose import detector_to_simple_pose, heatmap_to_coord\n",
    "from gluoncv.utils.viz import cv_plot_image, cv_plot_keypoints, plot_image\n",
    "\n",
    "from gluoncv import model_zoo, data, utils\n",
    "from gluoncv.data.transforms.pose import detector_to_alpha_pose, heatmap_to_coord_alpha_pose\n",
    "\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b3d59",
   "metadata": {},
   "source": [
    "Helper Functions - Action Classification - Neural Net Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c897f6d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:33:27.881814Z",
     "start_time": "2022-04-27T14:33:27.850713Z"
    }
   },
   "outputs": [],
   "source": [
    "def pose_normalization(x):\n",
    "    def retrain_only_body_joints(x_input):\n",
    "        x0 = x_input.copy()\n",
    "        #x0 = x0[2:2+13*2]\n",
    "        x0 = x0[2:2+13*2] # disregards face-related points\n",
    "        return x0\n",
    "\n",
    "    def normalize(x_input):\n",
    "        # Separate original data into x_list and y_list\n",
    "        lx = []\n",
    "        ly = []\n",
    "        N = len(x_input)\n",
    "        i = 0\n",
    "        while i<N:\n",
    "            lx.append(x_input[i])\n",
    "            ly.append(x_input[i+1])\n",
    "            i+=2\n",
    "        lx = np.array(lx)\n",
    "        ly = np.array(ly)\n",
    "\n",
    "        # Get rid of undetected data (=0)\n",
    "        non_zero_x = []\n",
    "        non_zero_y = []\n",
    "        for i in range(int(N/2)):\n",
    "            if lx[i] != 0:\n",
    "                non_zero_x.append(lx[i])\n",
    "            if ly[i] != 0:\n",
    "                non_zero_y.append(ly[i])\n",
    "        if len(non_zero_x) == 0 or len(non_zero_y) == 0:\n",
    "            return np.array([0] * N)\n",
    "\n",
    "        # Normalization x/y data according to the bounding box\n",
    "        origin_x = np.min(non_zero_x)\n",
    "        origin_y = np.min(non_zero_y)\n",
    "        len_x = np.max(non_zero_x) - np.min(non_zero_x)\n",
    "        len_y = np.max(non_zero_y) - np.min(non_zero_y)\n",
    "        x_new = []\n",
    "        for i in range(int(N/2)):\n",
    "            if (lx[i] + ly[i]) == 0:\n",
    "                x_new.append(-1)\n",
    "                x_new.append(-1)\n",
    "            else:\n",
    "                x_new.append((lx[i] - origin_x) / len_x)\n",
    "                x_new.append((ly[i] - origin_y) / len_y)\n",
    "        return x_new\n",
    "\n",
    "    x_body_joints_xy = retrain_only_body_joints(x)\n",
    "    x_body_joints_xy = normalize(x_body_joints_xy)\n",
    "    return x_body_joints_xy\n",
    "\n",
    "def drawActionResult(img_display, skeleton, str_action_type):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "    minx = 999\n",
    "    miny = 999\n",
    "    maxx = -999\n",
    "    maxy = -999\n",
    "    i = 0\n",
    "    NaN = 0\n",
    "\n",
    "    while i < len(skeleton):\n",
    "        if not(skeleton[i]==NaN or skeleton[i+1]==NaN):\n",
    "            minx = min(minx, skeleton[i])\n",
    "            maxx = max(maxx, skeleton[i])\n",
    "            miny = min(miny, skeleton[i+1])\n",
    "            maxy = max(maxy, skeleton[i+1])\n",
    "        i+=2\n",
    "\n",
    "    minx = int(minx * img_display.shape[1])\n",
    "    miny = int(miny * img_display.shape[0])\n",
    "    maxx = int(maxx * img_display.shape[1])\n",
    "    maxy = int(maxy * img_display.shape[0])\n",
    "    print(minx, miny, maxx, maxy)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    # drawBoxToImage(img_display, [minx, miny], [maxx, maxy])\n",
    "    img_display = cv2.rectangle(img_display,(minx, miny),(maxx, maxy),(0,255,0), 4)\n",
    "\n",
    "    # Draw text at left corner\n",
    "\n",
    "\n",
    "    box_scale = max(0.5, min(2.0, (1.0*(maxx - minx)/img_display.shape[1] / (0.3))**(0.5) ))\n",
    "    fontsize = 1.5 * box_scale\n",
    "    linewidth = int(math.ceil(3 * box_scale))\n",
    "\n",
    "    TEST_COL = int( minx + 5 * box_scale)\n",
    "    TEST_ROW = int( miny - 10 * box_scale)\n",
    "\n",
    "    img_display = cv2.putText(img_display, str_action_type, (TEST_COL, TEST_ROW), font, fontsize, (0, 0, 255), linewidth, cv2.LINE_AA)\n",
    "\n",
    "    return img_display\n",
    "\n",
    "def match_pose_output_to_classifier_input(pred_coords):\n",
    "\n",
    "    alphapose_resnet_joint_odering_dict = {\"nose\":0, \n",
    "                                        \"left_eye\":0,\n",
    "                                        \"right_eye\":0,\n",
    "                                        \"left_ear\":0,\n",
    "                                        \"right_ear\":0,\n",
    "                                        \"left_shoulder\":0,\n",
    "                                        \"right_shoulder\":0,\n",
    "                                        \"left_elbow\":0,\n",
    "                                        \"right_elbow\":0,\n",
    "                                        \"left_wrist\":0,\n",
    "                                        \"right_wrist\":0,\n",
    "                                        \"left_hip\":0,\n",
    "                                        \"right_hip\":0,\n",
    "                                        \"left_knee\":0,\n",
    "                                        \"right_knee\":0,\n",
    "                                        \"left_ankle\":0,\n",
    "                                        \"right_ankle\":0}  \n",
    "\n",
    "    tf_pose_est_joint_odering_dict =      {\"nose\":0, \n",
    "                                        \"neck\":0,\n",
    "                                        \"right_shoulder\":0,\n",
    "                                        \"right_elbow\":0,\n",
    "                                        \"right_wrist\":0,\n",
    "                                        \"left_shoulder\":0,\n",
    "                                        \"left_elbow\":0,\n",
    "                                        \"left_wrist\":0,\n",
    "                                        \"right_hip\":0,\n",
    "                                        \"right_knee\":0,\n",
    "                                        \"right_ankle\":0,\n",
    "                                        \"left_hip\":0,\n",
    "                                        \"left_knee\":0,\n",
    "                                        \"left_ankle\":0,\n",
    "                                        \"right_eye\":0,\n",
    "                                        \"left_eye\":0,\n",
    "                                        \"right_ear\":0,\n",
    "                                        \"left_ear\":0}  \n",
    "\n",
    "    for i,key in enumerate(alphapose_resnet_joint_odering_dict):\n",
    "        alphapose_resnet_joint_odering_dict[key] = pred_coords[i].asnumpy()\n",
    "    alphapose_resnet_joint_odering_dict[\"neck\"] = 0.5*(alphapose_resnet_joint_odering_dict[\"left_shoulder\"] + alphapose_resnet_joint_odering_dict[\"right_shoulder\"])\n",
    "    for key in tf_pose_est_joint_odering_dict.keys():\n",
    "        tf_pose_est_joint_odering_dict[key] = alphapose_resnet_joint_odering_dict[key]\n",
    "\n",
    "    skeleton_classifier_input = []\n",
    "\n",
    "    for value in tf_pose_est_joint_odering_dict.values():\n",
    "        skeleton_classifier_input.append(value[0])\n",
    "        skeleton_classifier_input.append(value[1])\n",
    "        \n",
    "    skeleton_classifier_input = np.array(skeleton_classifier_input)\n",
    "\n",
    "    return skeleton_classifier_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab063f83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:33:27.897817Z",
     "start_time": "2022-04-27T14:33:27.883814Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActionClassifier(object):\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        from keras.models import load_model\n",
    "\n",
    "        self.dnn_model = load_model(model_path)\n",
    "        self.action_dict = [\"kick\", \"punch\", \"squat\", \"stand\", \"wave\"]\n",
    "        #self.action_dict = [\"stand\", \"wave\"]\n",
    "\n",
    "    def predict(self, skeleton):\n",
    "\n",
    "        # Preprocess data\n",
    "        tmp = pose_normalization(skeleton)\n",
    "        skeleton_input = np.array(tmp).reshape(-1, len(tmp))\n",
    "            \n",
    "        # Predicted label: int & string\n",
    "        predicted_idx = np.argmax(self.dnn_model.predict(skeleton_input))\n",
    "        predicted_label = self.action_dict[predicted_idx]\n",
    "\n",
    "        return predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e0049",
   "metadata": {},
   "source": [
    "Helper Functions - Action Classification - Angle Calculation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3232766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:33:27.913821Z",
     "start_time": "2022-04-27T14:33:27.899818Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateAngle(landmark1, landmark2, landmark3):\n",
    "    '''\n",
    "    This function calculates angle between three different landmarks.\n",
    "    Args:\n",
    "        landmark1: The first landmark containing the x,y and z coordinates.\n",
    "        landmark2: The second landmark containing the x,y and z coordinates.\n",
    "        landmark3: The third landmark containing the x,y and z coordinates.\n",
    "    Returns:\n",
    "        angle: The calculated angle between the three landmarks.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Get the required landmarks coordinates.\n",
    "    x1, y1 = landmark1\n",
    "    x2, y2 = landmark2\n",
    "    x3, y3 = landmark3\n",
    "\n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "    angle = np.abs(angle)\n",
    "    # Check if the angle is less than zero.\n",
    "    if angle > 180.0:\n",
    "\n",
    "        angle = 360-angle\n",
    "    \n",
    "    # Return the calculated angle.\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513f2959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:33:27.929825Z",
     "start_time": "2022-04-27T14:33:27.915822Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifyPose(kp_array, output_image=None, display=False):\n",
    "    '''\n",
    "    This function classifies yoga poses depending upon the angles of various body joints.\n",
    "    Args:\n",
    "        kp_array: A list of detected landmarks of the person whose pose needs to be classified.\n",
    "        output_image: A image of the person with the detected pose landmarks drawn.\n",
    "        display: A boolean value that is if set to true the function displays the resultant image with the pose label \n",
    "        written on it and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The image with the detected pose landmarks drawn and pose label written.\n",
    "        label: The classified pose label of the person in the output_image.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Initialize the label of the pose. It is not known at this stage.\n",
    "    label = 'Unknown Pose'\n",
    "\n",
    "    # Specify the color (Red) with which the label will be written on the image.\n",
    "    color = (0, 0, 255)\n",
    "    \n",
    "    # Calculate the required angles.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Get the angle between the left shoulder, elbow and wrist points. \n",
    "    left_elbow_angle = calculateAngle(kp_array[5],\n",
    "                                      kp_array[7],\n",
    "                                      kp_array[9])\n",
    "    \n",
    "    # Get the angle between the right shoulder, elbow and wrist points.\n",
    "    right_elbow_angle = calculateAngle(kp_array[6],\n",
    "                                       kp_array[8],\n",
    "                                       kp_array[10])\n",
    "    \n",
    "    # Get the angle between the left elbow, shoulder and hip points.\n",
    "    left_shoulder_angle = calculateAngle(kp_array[7],\n",
    "                                         kp_array[5],\n",
    "                                         kp_array[11])\n",
    "    # Get the angle between the right hip, shoulder and elbow points.\n",
    "    right_shoulder_angle = calculateAngle(kp_array[12],\n",
    "                                          kp_array[6],\n",
    "                                          kp_array[8])\n",
    "    \n",
    "    #print('left_elbow_angle: ', left_elbow_angle ,'\\n right_elbow_angle: ', right_elbow_angle)\n",
    "    #print('left_shoulder_angle: ', left_shoulder_angle ,'\\n right_shoulder_angle: ', right_shoulder_angle)\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the warrior II pose or the T pose.\n",
    "    # As for both of them, both arms should be straight and shoulders should be at the specific angle.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if the both arms are straight.\n",
    "    if left_elbow_angle > 125 and left_elbow_angle < 220 and right_elbow_angle > 125 and right_elbow_angle < 220:\n",
    "        #label = 'T Pose'\n",
    "        # Check if shoulders are at the required angle.\n",
    "        if left_shoulder_angle > 70 and left_shoulder_angle < 110 and right_shoulder_angle > 70 and right_shoulder_angle < 110:\n",
    "            label = 'T Pose'\n",
    "\n",
    "    if right_elbow_angle > 50 and right_elbow_angle < 130 and right_shoulder_angle > 70 and right_shoulder_angle < 110:\n",
    "        label = 'Power to the People'                  \n",
    "    \n",
    "    # Check if the pose is classified successfully\n",
    "    if label != 'Unknown Pose':\n",
    "        \n",
    "        # Update the color (to green) with which the label will be written on the image.\n",
    "        color = (0, 255, 0)  \n",
    "    \n",
    "    # Write the label on the output image. \n",
    "    #cv2.putText(output_image, label, (10, 30),cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "    \n",
    "    # Check if the resultant image is specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the resultant image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the classified label.\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936249a0",
   "metadata": {},
   "source": [
    "Load Models: Human Detectors, Pose Estimators, Action Classifers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03596311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:33:28.752428Z",
     "start_time": "2022-04-27T14:33:27.930825Z"
    }
   },
   "outputs": [],
   "source": [
    "detector_slow = model_zoo.get_model('faster_rcnn_resnet50_v1b_voc', pretrained=True)\n",
    "pose_net_slow = model_zoo.get_model('alpha_pose_resnet101_v1b_coco', pretrained=True)\n",
    "detector_slow.reset_class([\"person\"], reuse_weights=['person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0adf0b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:33:29.420463Z",
     "start_time": "2022-04-27T14:33:28.754413Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIG AL\\anaconda3\\envs\\DLAV\\lib\\site-packages\\mxnet\\gluon\\block.py:1512: UserWarning: Cannot decide type for the following arguments. Consider providing them as input:\n",
      "\tdata: None\n",
      "  input_sym_arg_type = in_param.infer_type()[0]\n"
     ]
    }
   ],
   "source": [
    "ctx = mx.cpu()\n",
    "#ctx = mx.gpu()\n",
    "detector_name = \"ssd_512_mobilenet1.0_coco\"\n",
    "detector_fast = get_model(detector_name, pretrained=True, ctx=ctx)\n",
    "\n",
    "detector_fast.reset_class(classes=['person'], reuse_weights={'person':'person'})\n",
    "detector_fast.hybridize()\n",
    "\n",
    "pose_net_fast = get_model('simple_pose_resnet18_v1b', pretrained='ccd24037', ctx=ctx)\n",
    "pose_net_fast.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b629291f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:33:29.438480Z",
     "start_time": "2022-04-27T14:33:29.422483Z"
    }
   },
   "outputs": [],
   "source": [
    "#action_classifier_net = ActionClassifier('/content/gdrive/MyDrive/Colab_Notebooks/DLAV/Project/tf-pose-estimation-master/action_recognition_model/action_recognition.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22492dee",
   "metadata": {},
   "source": [
    "Run Pipeline on Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81149291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:46:43.183025Z",
     "start_time": "2022-04-27T14:46:19.380689Z"
    }
   },
   "outputs": [],
   "source": [
    "axes = None\n",
    "Fast = True\n",
    "\n",
    "#trigger = 'Power to the People'\n",
    "trigger = 'Power to the People'\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(1)  ### letting the camera autofocus\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = mx.nd.array(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).astype('uint8')\n",
    "    t1 = time.time()\n",
    "\n",
    "    if Fast == True:\n",
    "        \n",
    "        x, frame = gcv.data.transforms.presets.ssd.transform_test(frame, short=512, max_size=350)\n",
    "        x = x.as_in_context(ctx)\n",
    "        class_IDs, scores, bounding_boxs = detector_fast(x)\n",
    "\n",
    "        pose_input, upscale_bbox = detector_to_simple_pose(frame, class_IDs, scores, bounding_boxs,\n",
    "                                                           output_shape=(128, 96), ctx=ctx)\n",
    "        if len(upscale_bbox) > 0:\n",
    "            \n",
    "            predicted_heatmap = pose_net_fast(pose_input)\n",
    "            pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)\n",
    "\n",
    "            img = cv_plot_keypoints(frame, pred_coords, confidence, class_IDs, bounding_boxs, scores,\n",
    "                        box_thresh=1, keypoint_thresh=0.2) \n",
    "            \n",
    "            for idx, skeleton in enumerate(pred_coords):\n",
    "                \n",
    "                action_label = classifyPose(skeleton.asnumpy()) \n",
    "                if action_label == trigger and scores[0, idx]>0.8:\n",
    "                    \n",
    "                    bbox_trigger = bounding_boxs[0].asnumpy()\n",
    "                    top_left = (int(bbox_trigger[idx,0]),int(bbox_trigger[idx,1]))\n",
    "                    bottom_right = (int(bbox_trigger[idx,2]),int(bbox_trigger[idx,3]))\n",
    "                    img = cv2.rectangle(img, top_left, bottom_right, (0,255,0), 2)\n",
    "                    cv2.putText(img, 'Triggered', (top_left[0], top_left[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 2)\n",
    "                \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        x, frame = gcv.data.transforms.presets.rcnn.transform_test(frame)\n",
    "\n",
    "        x = x.as_in_context(ctx)\n",
    "        class_IDs, scores, bounding_boxs = detector_slow(x)\n",
    "        pose_input, upscale_bbox = detector_to_alpha_pose(frame, class_IDs, scores, bounding_boxs, ctx=ctx) # output_shape=(128, 96)\n",
    "        \n",
    "        if len(upscale_bbox) > 0:\n",
    "            \n",
    "            predicted_heatmap = pose_net_slow(pose_input)\n",
    "            pred_coords, confidence = heatmap_to_coord_alpha_pose(predicted_heatmap, upscale_bbox)\n",
    "\n",
    "            img = cv_plot_keypoints(frame, pred_coords, confidence, class_IDs, bounding_boxs, scores,\n",
    "                                    box_thresh=0.5, keypoint_thresh=0.2)\n",
    "            \n",
    "            for idx, skeleton in enumerate(pred_coords):\n",
    "                action_label = classifyPose(skeleton.asnumpy()) \n",
    "                if action_label == trigger:\n",
    "\n",
    "                    bbox_trigger = bounding_boxs[0].asnumpy()\n",
    "                    top_left = (int(bbox_trigger[idx,0]),int(bbox_trigger[idx,1]))\n",
    "                    bottom_right = (int(bbox_trigger[idx,2]),int(bbox_trigger[idx,3]))\n",
    "                    img = cv2.rectangle(img, top_left, bottom_right, (0,255,0), 2)\n",
    "                    cv2.putText(img, 'Triggered', (top_left[0], top_left[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 2)\n",
    "\n",
    "            \n",
    "    cv_plot_image(img)\n",
    "    \n",
    "    k = cv2.waitKey(33)\n",
    "    if k==27:      # Esc key to stop\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "614e18c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:39:09.446011Z",
     "start_time": "2022-04-27T14:39:09.423714Z"
    }
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLAV",
   "language": "python",
   "name": "dlav"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
